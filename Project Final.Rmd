---
title: "Effects of Latitude and Elevation on Temperature"
author: "Jina Park - 30129680"
date: "4/10/2022"
output: pdf_document
---

```{r setup, include=FALSE}
library(mosaic)
library(ggplot2)
library("ggpubr")
library(readxl)
library(coda)
library(MASS)
library(AICcmodavg)
```

----

**Abstract**

Bayes' Theorem uses the prior probability distribution and the likelihood of the data to generate a posterior probability distribution. Models, hypotheses, and parameter estimates can be stated with a function of posterior probability distributions.

This paper is collecting the data from 16 counties in Texas and will mainly be addressing the effects of latitude and elevation (altitude) on temperature using Bayesian inference in order to find the relationship between them and to make temperature forecasts depending on latitude and elevation. In addition, metropolis algorithm and non-informative prior distribution are expected to be used and compared to draw a conclusion.

----

**Introduction**

Bayes' Theorem is widely used to predict posterior distributions using the prior distribution and the likelihood of the data, treating model parameters as random variables. With Bayesian inference, it is not only can obtain a quantitative measure of the probability of a hypothesis being true in light of the available data (P(H|Y)), but also figure out probability as an individual's degree of belief in the likelihood of an event. The probability of the hypothesis being conditional on the sample data can be found by the following formula:


$P(H \mid Y)$ = $\frac{f(Y \mid H)\pi(H)}{P(Y)}$


It is well known that change in latitude and altitude affects on temperature. The lower latitude and elevation, the warmer the temperature (Key et al. 1997). When we are able to figure out the relationship of temperature with latitude and altitude, we can predict temperature with corresponding latitude and altitude with certain percentage of confidence. Therefore, this paper will be using Bayesian inference to observe the effects of latitude and elevation on temperature by taking the data that measures average high temperature in January of 16 counties in Texas with latitude and altitude level as an example, and forecast the temperature in January in 16 counties of Texas.

We are expecting to obtain the best model with Aikaike's information criterion (AIC; Burnham & Anderson 2002), maximum likelihood estimates of the model and posterior distribution with non-informative prior distribution. We will assume non-informative prior distribution as $\beta$ ~ *N* (0,1000) to approximate the posterior distribution.

I will be using RStudio throughout the paper to conduct parameter estimation and model selection. The best model will be chosen from the set of models by Aikaike's information criterion (Burnham & Anderson 2002). Metropolis algorithm and non-informative prior distribution will be used to estimate the posterior probability distribution.

----

**Data and Analysis**

The original data set (University of Florida 2013) not only includes variables latitude and elevation but also longitude, however, longitude has no relationship (no effect) with temperature change((National Geographic Society 2019) so longitude is excluded from the data to be worked with.
Hence, the working data will be the following:

```{r,echo=TRUE}
texas <- read_excel("texas.dat.xlsx", 
                        range = "A1:D17")
texas
```

Out of this data set, I examined simple additive models (temperature as a function of latitude and elevation) and models that included all possible interaction terms. The 'best' model was chosen from the set of candidate models by minimizing Aikaike's information criterion (AIC; Burnham & Anderson 2002):


AIC = -2 $\log(x)$ $($*L*$(\hat{\beta} \mid data))$ +2*k*


$($*L*$(\hat{\beta} \mid data))$ stands for the likelihood of the model with parameters $\beta_i$ conditional on the data, and *k* stands for the number of parameters in the model.

The response variable is the average Jan high temp in 16 Texas counties.
Here, It uses a linear model to relate a discrete (Poission) random variable (temperature) to two continuous predictor variables (latitude and elevation).

```{r,echo=TRUE}
lat.mod <- lm(texas$`Temp (F)`~texas$Latitude, data = texas)
ele.mod <-  lm(texas$`Temp (F)`~texas$Elevation, data =texas)
lat.ele.mod <- lm(texas$`Temp (F)`~texas$Latitude+texas$Elevation, data = texas)
interaction.mod <- lm(texas$`Temp (F)`~texas$Latitude*texas$Elevation, data = texas)
all.mod <- lm(texas$`Temp (F)`~texas$Latitude+texas$Elevation+texas$Latitude*texas$Elevation, data = texas)

models <- list(lat.mod,ele.mod,lat.ele.mod,interaction.mod,all.mod)
model.names <- c('L','E','L+E','LxE','L+E+LxE')

aictab(cand.set=models, modnames = model.names)
```

In the table above, L indicates latitude and E indicates elevation. According to this data, the smallest value of AICc is 48.54, which has a simple addictive model T = L+E. Therefore, we will be working on the model that has an intercept ($\beta_1$) and main effects ($\beta_2$,$\beta_3$) without interaction term:


**($log(\hat{T}_i)$) = $\hat\beta_1$ + $\hat\beta_2$$latitude_i$ + $\hat\beta_3$$elevation_i$**


The fit of this model enables us to understand the relationship of temperature with latitude and elevation, and it can be illustrated with graphs.

```{r,echo=TRUE}
ggline(data=texas, x="Latitude", y="Temp (F)")
ggline(data=texas, x="Elevation", y="Temp (F)")
```

As can be seen from the graphs above, they provide us with the information that the higher latitude and elevation, the lower the temperature. Also, the following graph and the ordinary least square(OLS) estimating table illustrate that the model fits nicely.

```{r,echo=TRUE}
ggplot(texas, aes(Latitude+Elevation, `Temp (F)`)) + geom_point()+geom_smooth(method = lm)
```

```{r,echo=TRUE}
#The ordinary least square (OLS) estimator
lmfit<-lm(texas$`Temp (F)`~texas$Latitude+texas$Elevation)
summary(lmfit)
```

By using the first equation stated earlier, we can derive the following equation to estimate the posterior:

$P(\beta \mid data)$ $\propto$ $P(data \mid \beta)$ X $\pi(H)$

We can figure out the maximum likelihood estimates (MLE) using the following R codes.

```{r,echo=TRUE}
texas.mle=glm(texas$`Temp (F)`~texas$Latitude+texas$Elevation, family="poisson")
summary(texas.mle)
```

Now, we are going to run Metropolis algorithm.

```{r,echo=FALSE}
y<-texas$`Temp (F)`
X<-cbind(rep(1,length(y)),texas$Latitude,texas$Elevation)
yX<-cbind(y,X)
colnames(yX)<-c("temperature","intercept","latitude","elevation")
n<-length(y)
p<-dim(X)[2]
pmn.beta<-rep(0,p)
psd.beta<-rep(10,p)
var.prop<-var(log(y+1/2))*solve(t(X)%*%X)
beta<-rep(0,p)
S <- 25000
BETA <- matrix(0,nrow=S, ncol=p)
ac<-0
set.seed(1)
#Proposal function, sampling from the multivariate normal distribution
rmvnorm <-function(n,mu,Sigma)
{E<-matrix(rnorm(n*length(mu)),n,length(mu))
  t(t(E%*%chol(Sigma))+c(mu))
  }

#MCMC
for(s in 1:S) {
  beta.p <- t(rmvnorm(1,beta,var.prop))
  log.r <- sum(dpois(y,exp(X%*%beta.p),log=T))-sum(dpois(y,exp(X%*%beta),log=T))+
    sum(dnorm(beta.p,pmn.beta,psd.beta,log=T))-sum(dnorm(beta,pmn.beta,psd.beta,log=T))
  if(log(runif(1))<log.r) {beta<-beta.p ; ac<-ac+1}
  BETA[s,]<-beta}
#Acceptance rate
#cat(ac/S,"\n") = 0.4112

#Effective sample size
#apply(BETA,2,effectiveSize) = 1313.3506  552.0609 1513.4282

```

Metropolis algorithm gives an acceptance rate approximately 41%. Effective sample size are 1313.3506, 552.0609 and 1513.4282 for $\beta_1$,$\beta_2$ and $\beta_3$, respectively.

Plots of the Markov chain in $\beta_2$ and $\beta_3$ along with autocorrelation functions are illustrated below:

```{r,echo=TRUE}
#Beta2
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,3))

blabs <-c(expression(beta[1]),expression(beta[2]),expression(beta[3]))
j<-2 #beta2
thin <- c(1,seq(10,S,by=25))
plot(thin,BETA[thin,j],type="l",xlab="iteration",ylab=blabs[j])
abline(h=mean(BETA[,j]))
acf(BETA[,j],ci.col="gray",xlab="lag")
acf(BETA[thin,j],xlab="lag/25",ci.col="red")

#Beta3
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,3))

blabs <-c(expression(beta[1]),expression(beta[2]),expression(beta[3]))
j<-3 #beta3
thin <- c(1,seq(10,S,by=25))
plot(thin,BETA[thin,j],type="l",xlab="iteration",ylab=blabs[j])
abline(h=mean(BETA[,j]))
acf(BETA[,j],ci.col="gray",xlab="lag")
acf(BETA[thin,j],xlab="lag/25",ci.col="red")
```

The graphs with lag/25 in x-axis gives us the autocorrelation function of every 25th value of parameters from Markov chain. For $\beta_2$, Markov chain is highly correlated. However, we find there is a correlated errors for $\beta_1$. Therefore, we are going to generalize this regression model.

```{r,echo=TRUE}
n<-dim(texas)[1]
y<-texas$`Temp (F)`
X<-cbind(rep(1,length(y)),texas$Elevation) 
DY <- abs(outer((1:n),(1:n),"-"))
lmfit<-lm(y~-1+X)
beta<-lmfit$coef
s2<-summary(lmfit)$sigma^2
phi<-acf(lmfit$residuals,plot=FALSE)$acf[2]

nu0<-1
s20<-1
T0<-diag(1/1000,nrow=2)

set.seed(1)
S<-25000
odens<-S/1000
OUT<-NULL
ac<-0
par(mfrow=c(1,2))
for (s in 1:S)
{
  Cor<-phi^DY
  iCor<-solve(Cor)
  V.beta<-solve(t(X)%*%iCor%*%X/s2+T0)
  E.beta<-V.beta%*%(t(X)%*%iCor%*%y/s2)
  beta<-t(rmvnorm(1,E.beta,V.beta))

  s2<-1/rgamma(1,(nu0+n)/2,(nu0*s20+t(y-X%*%beta)%*%iCor%*%(y-X%*%beta))/2)

  phi.p<-abs(runif(1,phi-.1,phi+.1))
  phi.p<- min(phi.p,2-phi.p)
  lr<- -.5*(determinant(phi.p^DY,log=TRUE)$mod-
              determinant(phi^DY,log=TRUE)$mod+
            sum(diag((y-X%*%beta)%*%t(y-X%*%beta)%*%(solve(phi.p^DY)-solve(phi^DY))))/s2)

  if(log(runif(1))<lr) {phi<-phi.p ; ac<-ac+1}
  if(s%%odens==0)
  {#cat(s,ac/s,beta,s2,phi,"\n")
    OUT<-rbind(OUT,c(beta,s2,phi))
  }
}
  
OUT.25000<-OUT
#Effective sample size
#apply(OUT.25000,2,effectiveSize) = 1000.0000 1000.0000 463.9249  380.8224

#figure
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))
plot(OUT.25000[,4],xlab="scan/25",ylab=expression(rho),type="l")
acf(OUT.25000[,4],ci.col="red",xlab="lag/25")
  
```

Now, we have a highly correlated Markov chain. This suggests that we have nearly the equivalent of 1000 independent samples of $\beta_2$ and $\beta_3$ with which to approximate the posterior distribution. We can visualize the MCMC approximations. MCMC approximations to the posterior marginal distributions of $\beta_2$ and $\beta_3$ are illustrated in black.

```{r,echo=TRUE}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
par(mfrow=c(1,2))

plot(density(BETA[,2],adj=2),lwd=2,type="l",xlab=expression(beta[2]),ylab=expression(paste(italic("p("),beta[2],"|",y,")")))
abline(h=0,col="gray")
abline(v=0,col="gray")
plot(density(BETA[,3],adj=2),lwd=2,type="l",xlab=expression(beta[3]),ylab=expression(paste(italic("p("),beta[2],"|",y,")")))
abline(h=0,col="gray")
abline(v=0,col="gray")

```

Apart from MCMC approximation, we can also work on non-informative prior to approximate the posterior distribution. I will be using a generalized linear regression model, assuming non-informative prior distribution as $\beta$ ~ *N* (0,1000). 

```{r,echo=TRUE}

n=16
nu0=1
s20=summary(lm(y~-1+X))$sigma^2

lm.gprior<-function(y,X,g=dim(X)[1],nu0=1,s20=try(summary(lm(y~-1+X))$sigma^2,silent=TRUE),S=1000)
{
  n<-dim(X)[1]
  p<-3
  Hg<-(g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)
  SSRg<-t(y)%*%(diag(1,nrow=n)-Hg)%*%y
  s2<-1/rgamma(S,(nu0+n)/2, (nu0*s20+SSRg)/2)
  
  Vb<-g*solve(t(X)%*%X)/(g+1)
  Eb<-Vb%*%t(X)%*%y
  E<-matrix(rnorm(S*p,0,sqrt(s2)),S,p)
  beta<-t(t(E%*%chol(Vb))+c(Eb))
  list(beta=beta,s2=s2)
}
x1<-texas$Latitude
x2<-texas$Elevation
y<-texas$`Temp (F)`
n<-length(y)
X<-cbind(rep(1,n),x1,x2)
p<-dim(X)[2]
tmp<-lm.gprior(y,X)
beta.post<-tmp$beta
beta.ols <-lm(y~-1+X)$coef
beta.ols #OLS beta estimates

g<-n
nu0<-1
s20<-summary(lm(y~-1+X))$sigma^2
beta.ols*g/(g+1) #Posterior mean

iXX<-solve(t(X)%*%X)

#Figure
#Marginal prior distribution
mdt<-function(t,mu,sig,nu){
  gamma(.5*(nu+1))*(1+((t-mu)/sig)^2/nu)^(-.5+(nu+1))/(sqrt(nu*pi)*sig*gamma(nu/2))
}

par(mfrow=c(1,2),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
x<-seq(-85,130,length=200)
plot(density(beta.post[,2],adj=2),xlab=expression(beta[2]),main="",ylab="",lwd=2)
abline(v=0,col="grey")
lines(x,mdt(x,0,sqrt(n*s20*iXX[2,2]),nu0),col="red")
x<-seq(-5,5,length=100)
plot(density(beta.post[,3],adj=2),xlab=expression(beta[3]),main = "",ylab="",lwd=2)
abline(v=0,col="grey")
lines(x,mdt(x,0,sqrt(n*s20*iXX[3,3]),nu0),col="red")

```

----

**Conclusion**

The graphs with two different algorithm give us the similar posterior distribution. From the Bayesian analysis, we could conclude that the additive model is a believable description of how latitude and elevation can be used to predict temperature. We have approximately 41% of acceptance rate from Metropolis method. However, there was a limitation to the model selection as there can be other factors that influences the temperature changes. Since Bayes' Theorem allows us to estimate the posterior distribution with a less prior information or data, other models, even with less information, can also be used to estimate the effects of latitude and elevation on temperature. In addition, other factors also can be considered to estimate temperature.

----

**References**

- Burnham, K.P. & Anderson, D.R. (2002). Model Selection and Inference: A Practical Information-theoretic Approach, 2nd edn. SpringerVerlag, New York.

- Key, J. R., Collins, J. B., Fowler, C., & Stone, R. S. (1997). High-latitude surface temperature estimates from thermal satellite data. Remote Sensing of Environment, 61(2), 302-309.

- National Geographic Society. (2019, September 19). Latitude, longitude, and temperature. National Geographic Society. Retrieved April 11, 2022, from https://www.nationalgeographic.org/activity/latitude-longitude-temperature/#:~:text=Latitude%20and%20longitude%20make%20up,and%20cooler%20approaching%20the%20Poles. 

- University of Florida. (2013, October 29). Index of /~winner/data. (n.d.). Retrieved April 11, 2022, from https://users.stat.ufl.edu/~winner/data/texas1.dat

- Aaron M. Ellison. (2004, May 10). Bayesian inference in ecology - ellison - 2004 - ecology ... Retrieved April 12, 2022, from https://onlinelibrary.wiley.com/doi/full/10.1111/j.1461-0248.2004.00603.x 